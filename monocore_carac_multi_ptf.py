# ======================================================================
# File: monocore_charac_multi_ptf.py
# Role: Load a mutli-session CSV and generate a bar/table chart
# Is written with the results of stress_memory application in mind
# ======================================================================
# Context:
#       Showcases performances across multiple applications stressed by
#       multiple applications (each stressed has all stressors)
# Overview:
#       Given a multi-session CSV such as one generated by TH_40...py
#       computes mean_elongation and represent them as heatmap
#       for all stressed/stressor couples.
# ======================================================================
# Command-line:
#       python monocore_charac_multi_ptf.py -i <csvfile> -o <pictures_folder>
#       python monocore_charac_multi_ptf.py -h
# ======================================================================

import sys, getopt
import os
import pandas as pd

import plotly.graph_objs as go
import plotly.figure_factory as ff
import plotly.express as px

from plotly.subplots import make_subplots

from IPython.display import display


import plotly.io as pio
pio.renderers.default = "notebook"

import yaml
yaml_folder = "/home/dev/utils/targets_info/"

CHART_HTML_FNAME = "monocore_carac_multi_ptf.html"
TH_PROG_USAGE= "python monocore_carac_multi_ptf.py -i <JSON_results_folder> -o <pictures_folder>"

nano_to_milli = 0.000001

def command_print_usage():
    print(TH_PROG_USAGE)

import os
import sys
import getopt
import pandas as pd

def main_get_options(argv):
    is_valid_i = False
    is_valid_o = False
    JSON_results_folder = ''
    pictures_folder = ''
    try:
        opts, _ = getopt.getopt(argv, "hi:o:t:", ["ifolder", "ofolder"])
    except getopt.GetoptError:
        command_print_usage()
        sys.exit(2)
    for opt, arg in opts:
        if opt == '-h':
            command_print_usage()
            sys.exit()
        elif opt in ("-i", "--ifolder"):
            JSON_results_folder = arg
            is_valid_i = True
        elif opt in ("-o", "--ofolder"):
            pictures_folder = arg
            is_valid_o = True
    return is_valid_i and is_valid_o, JSON_results_folder, pictures_folder

if __name__ == "__main__":
    # Récupérer les arguments de ligne de commande
    is_valid, JSON_results_folder, pictures_folder = main_get_options(sys.argv[1:])

    if not is_valid:
        print("[ERROR] invalid command.\n")
        command_print_usage()
        exit()

    # Dictionnaire pour stocker chaque DataFrame par nom de fichier
    dataframes = {}
    previous_df = None

    merged_dataframes_columns = ['core0AppElfId']
    merged_dataframes = pd.DataFrame(columns=merged_dataframes_columns)

    # Lire chaque fichier CSV commençant par 'TH40_' dans le dossier spécifié
    for filename in os.listdir(JSON_results_folder):
        if filename.startswith("TH40_") and filename.endswith(".csv"):
            file_path = os.path.join(JSON_results_folder, filename)
            
            # Créer un DataFrame pour chaque fichier
            csv_types = {
                "a_appli_name": "string",
                "b_appli_name": "string",
                "c_appli_name": "string",
                "d_appli_name": "string"
            }
            dataframes[filename] = pd.read_csv(file_path, dtype=csv_types)

    # Iterate over database
    for name, dataframe in dataframes.items():

        # Switch based on filename
        if 'imx8x' in name:
            target = 1
            nickname = 'imx8x'
        elif 'ls1028' in name:
            target = 3
            nickname = 'ls1028'
        else :
            print(f"No target found for ", name)
        # Recover yaml config from target id
        config = None
        with open(yaml_folder + str(target) + ".yml", 'r') as file:
            config = yaml.safe_load(file)

        main_columns = ["core0AppElf", "core1AppElf", "core0AppId", "core1AppId", \
            "evt_id", "part_id", "is_begin_not_end", "measurement_value", "activation", "obs_point_name"]
        if (not all(col in dataframe.columns for col in main_columns)):
            print("[ERROR] csv does not have all columns necessary, main_columns:", main_columns, "dataframe columns: ", dataframe.columns)
            quit()
        extra_columns = [col for col in dataframe.columns if col not in main_columns]

        # remove columns with only NaN on all rows as they bring forward no data 
        # dataframe[extra_columns].dropna(axis=1, how="all", inplace=True)
        dataframe = dataframe[dataframe["measurement_value"] != -1]
        
        # make unique string value for each Application (core*AppId are not unique by default) 
        dataframe["core0AppElfId"] = dataframe["core0AppElf"] + "_" + dataframe["core0AppId"].astype(str)
        dataframe["core1AppElfId"] = dataframe["core1AppElf"] + "_" + dataframe["core1AppId"].astype(str)
        # core0AppElf and core1AppElf columns are redondant now
        dataframe = dataframe.drop(columns=["core0AppElf", "core1AppElf"])

        dataframe = dataframe.drop(columns="obs_point_id") # obs_point_id is redondant with evt_id and is_begin_not_end combined 

        dataframe = dataframe[(dataframe["evt_id"] == "ticks") & (dataframe["part_id"]==3)] 

        # Make is_begin_not_end column a list of columns to prepare for difference computing (unfitting column/row combination get value nan)
        columns = list(dataframe.columns)
        index_columns = [col for col in columns if col not in ["is_begin_not_end", "measurement_value"]]
        index_columns2 = index_columns[:]
        index_columns2.append("is_begin_not_end")
        dataframe_dupli = dataframe[dataframe.duplicated(subset=index_columns2, keep=False)]
        dataframe_dupli.to_csv(pictures_folder + "/dupli_results.csv")



        dataframe = dataframe.pivot(index=index_columns,  
                    columns="is_begin_not_end", values="measurement_value") \
                    .reset_index() # Original columns were squished inside the index so reset_index unsquishes them
                    
        # Compute new column containing difference of end tick time and beg tick time 
        dataframe["end-begin"] = dataframe[0] - dataframe[1]

        # Remerge the is_begin_not_end inside a single column
        dataframe = dataframe.melt(id_vars=index_columns,
                    value_name="measurement_value", var_name="is_begin_not_end") \
                    .dropna(subset=["measurement_value"]) # Remove nan value from unfitting column/row combination of pivot

        # Filter for end-begin, ticks, PERPRO_TIMWIN_0, 0 limiter_cores_checked
        dataframe = dataframe[(dataframe["is_begin_not_end"] == "end-begin") & (dataframe["evt_id"] == "ticks") & (dataframe["obs_point_name"] == "PERPRO_TIMWIN_0") & (dataframe["limiter_cores_checked"] == 0)]
        # Remove column that bring no extra information
        dataframe = dataframe.drop(columns=["is_begin_not_end", "evt_id", "obs_point_name", "limiter_cores_checked"])

        columns = list(dataframe.columns)
        index_columns = [col for col in columns if col not in ["activation", "measurement_value"]]
        # Make summarising stats (mean) for different activation of otherwise same column values
        dataframe = dataframe.groupby(index_columns, dropna=False) \
                    .agg(mean_value=("measurement_value", "mean")) \
                    .reset_index() # Original columns were squished by groupby so reset_index unsquishes them

        dataframe["mean_value"] = dataframe["mean_value"] * nano_to_milli * config["ticker_period"]

        # Separate dataframe into ref_dataframe with only core0 active and dataframe with both core actives
        dataframe = dataframe[dataframe["core1AppId"] == 0]

        # # Recover only app test id and mean_value 
        # dataframe = dataframe[["core0AppElfId","mean_value"]]
        # # Sort by appli id 
        # dataframe = dataframe.sort_values(by='core0AppElfId')
        dataframe = dataframe[["core0AppElfId","mean_value"]]
        dataframe = dataframe.rename(columns={"mean_value": f"mean_value_{nickname}"})

        
        if previous_df is not None: 
            merged_df =dataframe.merge(previous_df, on='core0AppElfId') 
            print(merged_df.head())
        previous_df = dataframe

    # # Initialize a figure with ff.create_table(table_data)
    colorscale = [[0, '#4d004c'],[.5, '#f2e5ff'],[1, '#ffffff']]
    fig = ff.create_table(merged_df, colorscale=colorscale)

    fig.add_trace(go.Bar(x=merged_df['core0AppElfId'], y=merged_df.iloc[:, 1], xaxis='x2', yaxis='y2',
                marker=dict(color='#ff6b33'),
                name=merged_df.columns[1]))

    fig.add_trace(go.Bar(x=merged_df['core0AppElfId'], y=merged_df.iloc[:, 2], xaxis='x2', yaxis='y2',
                    marker=dict(color='#99d15e'),
                    name=merged_df.columns[2]))
    
    fig.update_layout(
        title_text = 'Multi Platform Comparison on monocore carac algorithm',
        height = 600,
        width = 800,
        margin = {'t':75, 'l':50},
        yaxis = {'domain': [0, .45]},
        xaxis2 = {'anchor': 'y2'},
        yaxis2 = {'domain': [.6, 1], 'anchor': 'x2', 'title': 'Time Execution (in ms)'}
    )
    html_content = fig.to_html(full_html=True, include_plotlyjs='cdn')
    html_content = html_content.replace(
        '<head>',
        '<head><style>body { display: flex; justify-content: center; margin: 0; }</style>'
    )
    # fig.write_html(pictures_folder + '/' + CHART_HTML_FNAME)
    # # Write html content 
    with open(pictures_folder + '/' + CHART_HTML_FNAME, 'w') as f:
        f.write(html_content)
